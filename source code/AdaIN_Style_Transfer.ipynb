{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztyFKHfJmmbS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "content_dir = \"dataset/content\"\n",
        "style_dir = \"dataset/style\"\n",
        "save_decoder_path = \"decoder.pth\"\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 8\n",
        "lr = 1e-4\n",
        "num_epochs = 10\n",
        "content_weight = 1.0\n",
        "style_weight = 10.0\n",
        "\n",
        "image_size = 256\n"
      ],
      "metadata": {
        "id": "zDtk8shrmr6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1, 3, 1, 1)\n",
        "vgg_std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1, 3, 1, 1)\n",
        "\n",
        "def vgg_preprocess(img):\n",
        "    # img is a PIL image or a tensor in [0,1]\n",
        "    # transform to tensor and normalize\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(img)\n",
        "\n",
        "def deprocess(tensor):\n",
        "    # Reverse of the preprocessing for display\n",
        "    tensor = tensor * vgg_std + vgg_mean\n",
        "    tensor = torch.clamp(tensor, 0, 1)\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "F2Fz8j0Bmr38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, content_dir, style_dir, transform=None):\n",
        "        self.content_images = glob.glob(os.path.join(content_dir, \"*\"))\n",
        "        self.style_images = glob.glob(os.path.join(style_dir, \"*\"))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.content_images), len(self.style_images))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        content_path = self.content_images[idx % len(self.content_images)]\n",
        "        style_path = self.style_images[idx % len(self.style_images)]\n",
        "\n",
        "        content_img = Image.open(content_path).convert(\"RGB\")\n",
        "        style_img = Image.open(style_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            content = self.transform(content_img)\n",
        "            style = self.transform(style_img)\n",
        "        else:\n",
        "            content = content_img\n",
        "            style = style_img\n",
        "\n",
        "        return content, style\n",
        "\n",
        "dataset = ImageDataset(content_dir, style_dir, transform=lambda img: vgg_preprocess(img))\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "07ODQYkFmr0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_variance_norm(feat, eps=1e-5):\n",
        "    mean = feat.mean(dim=[2, 3], keepdim=True)\n",
        "    var = feat.var(dim=[2, 3], keepdim=True) + eps\n",
        "    std = var.sqrt()\n",
        "    return mean, std\n",
        "\n",
        "def adain(content_feat, style_feat, eps=1e-5):\n",
        "    c_mean, c_std = mean_variance_norm(content_feat, eps)\n",
        "    s_mean, s_std = mean_variance_norm(style_feat, eps)\n",
        "    normalized = (content_feat - c_mean) / c_std\n",
        "    return normalized * s_std + s_mean\n",
        "\n",
        "# Encoder: VGG19 up to relu4_1\n",
        "def vgg_encoder():\n",
        "    vgg = models.vgg19(pretrained=True).features\n",
        "    # Freeze weights\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Extract up to relu4_1 (which is index 21: conv4_1 at 21 and relu4_1 at 22)\n",
        "    encoder = nn.Sequential()\n",
        "    for i in range(23):\n",
        "        encoder.add_module(str(i), vgg[i])\n",
        "    return encoder\n",
        "\n",
        "# Decoder (mirroring VGG up to relu4_1)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        # This is one possible decoder architecture (from AdaIN paper)\n",
        "        self.layers = nn.Sequential(\n",
        "            # Input is relu4_1 feature space: C=512\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(512, 512, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(512, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 128, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(128, 128, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(128, 64, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(64, 64, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(64, 3, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ],
      "metadata": {
        "id": "fkfX5hBImryv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_variance_norm(feat, eps=1e-5):\n",
        "    mean = feat.mean(dim=[2, 3], keepdim=True)\n",
        "    var = feat.var(dim=[2, 3], keepdim=True) + eps\n",
        "    std = var.sqrt()\n",
        "    return mean, std\n",
        "\n",
        "def adain(content_feat, style_feat, eps=1e-5):\n",
        "    c_mean, c_std = mean_variance_norm(content_feat, eps)\n",
        "    s_mean, s_std = mean_variance_norm(style_feat, eps)\n",
        "    normalized = (content_feat - c_mean) / c_std\n",
        "    return normalized * s_std + s_mean\n",
        "\n",
        "# Encoder: VGG19 up to relu4_1\n",
        "def vgg_encoder():\n",
        "    vgg = models.vgg19(pretrained=True).features\n",
        "    # Freeze weights\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Extract up to relu4_1 (which is index 21: conv4_1 at 21 and relu4_1 at 22)\n",
        "    encoder = nn.Sequential()\n",
        "    for i in range(23):\n",
        "        encoder.add_module(str(i), vgg[i])\n",
        "    return encoder\n",
        "\n",
        "# Decoder (mirroring VGG up to relu4_1)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        # This is one possible decoder architecture (from AdaIN paper)\n",
        "        self.layers = nn.Sequential(\n",
        "            # Input is relu4_1 feature space: C=512\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(512, 512, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(512, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 256, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(256, 128, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(128, 128, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(128, 64, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(64, 64, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d((1,1,1,1)),\n",
        "            nn.Conv2d(64, 3, 3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ],
      "metadata": {
        "id": "CZqB54Ckmroo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(x, model):\n",
        "    # We want multiple layers: relu1_1, relu2_1, relu3_1, relu4_1\n",
        "    # According to VGG19:\n",
        "    # relu1_1: index 1\n",
        "    # relu2_1: index 6\n",
        "    # relu3_1: index 11\n",
        "    # relu4_1: index 20 (conv4_1) or 21 (relu4_1) depending on indexing\n",
        "    # Actually for ease, let's just run step by step and store needed layers.\n",
        "    relu1_1 = model[:2](x)    # up to relu1_1\n",
        "    relu2_1 = model[:7](x)    # up to relu2_1\n",
        "    relu3_1 = model[:12](x)   # up to relu3_1\n",
        "    relu4_1 = model[:23](x)   # up to relu4_1\n",
        "    return [relu1_1, relu2_1, relu3_1, relu4_1]\n",
        "\n",
        "def calc_content_loss(out_feat, target_feat):\n",
        "    return torch.mean((out_feat - target_feat)**2)\n",
        "\n",
        "def calc_style_loss(out_feats, target_feats):\n",
        "    # style loss is computed by matching mean and variance at each layer\n",
        "    loss = 0.0\n",
        "    for out_f, tgt_f in zip(out_feats, target_feats):\n",
        "        out_mean, out_std = mean_variance_norm(out_f)\n",
        "        tgt_mean, tgt_std = mean_variance_norm(tgt_f)\n",
        "        loss += torch.mean((out_mean - tgt_mean)**2) + torch.mean((out_std - tgt_std)**2)\n",
        "    return loss\n",
        "\n",
        "############################################################\n",
        "# Training Setup\n",
        "############################################################\n",
        "\n",
        "decoder = Decoder().to(device)\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n"
      ],
      "metadata": {
        "id": "rjqd2bn2m6zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    decoder.train()\n",
        "    for i, (content_imgs, style_imgs) in enumerate(dataloader):\n",
        "        content_imgs = content_imgs.to(device)\n",
        "        style_imgs = style_imgs.to(device)\n",
        "\n",
        "        # Extract content and style features\n",
        "        with torch.no_grad():\n",
        "            content_feats = encoder(content_imgs)  # relu4_1 features\n",
        "            style_feats = encoder(style_imgs)\n",
        "\n",
        "        # Apply AdaIN\n",
        "        t = adain(content_feats, style_feats)\n",
        "        # Blend factor alpha if needed, here alpha=1 for training\n",
        "        t = t\n",
        "\n",
        "        # Decode\n",
        "        out = decoder(t)\n",
        "\n",
        "        # Compute losses\n",
        "        out_feats = encoder(out)\n",
        "\n",
        "        # Content loss (compare out_feats relu4_1 to content_feats relu4_1)\n",
        "        c_loss = calc_content_loss(out_feats, content_feats)\n",
        "\n",
        "        # Style loss (compare multiple layers)\n",
        "        out_feats_multi = extract_features(out, encoder)\n",
        "        style_feats_multi = extract_features(style_imgs, encoder)\n",
        "        s_loss = calc_style_loss(out_feats_multi, style_feats_multi)\n",
        "\n",
        "        total_loss = content_weight * c_loss + style_weight * s_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], \"\n",
        "                  f\"Content Loss: {c_loss.item():.4f}, Style Loss: {s_loss.item():.4f}, \"\n",
        "                  f\"Total Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "# Save trained decoder\n",
        "torch.save(decoder.state_dict(), save_decoder_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgCCezj7m61N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def style_transfer(encoder, decoder, content, style, alpha=1.0):\n",
        "    with torch.no_grad():\n",
        "        content_feat = encoder(content)\n",
        "        style_feat = encoder(style)\n",
        "        t = adain(content_feat, style_feat)\n",
        "        t = alpha * t + (1 - alpha) * content_feat\n",
        "        out = decoder(t)\n",
        "    return out\n",
        "\n",
        "# Example inference usage:\n",
        "# Load a content and style image (just as an example)\n",
        "content_image = Image.open(\"path_to_a_content_image.jpg\").convert(\"RGB\")\n",
        "style_image = Image.open(\"path_to_a_style_image.jpg\").convert(\"RGB\")\n",
        "\n",
        "content_tensor = vgg_preprocess(content_image).unsqueeze(0).to(device)\n",
        "style_tensor = vgg_preprocess(style_image).unsqueeze(0).to(device)\n",
        "\n",
        "decoder.eval()\n",
        "# Load trained decoder weights if needed\n",
        "# decoder.load_state_dict(torch.load(save_decoder_path))\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = style_transfer(encoder, decoder, content_tensor, style_tensor, alpha=1.0)\n",
        "\n",
        "# Convert output to PIL Image for saving\n",
        "out_img = deprocess(output[0].cpu())\n",
        "out_img_pil = transforms.ToPILImage()(out_img)\n",
        "out_img_pil.save(\"stylized_output.jpg\")\n",
        "\n",
        "print(\"Style transfer complete! Check stylized_output.jpg.\")\n"
      ],
      "metadata": {
        "id": "ZPS9Eh5Rm65L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}